# @package _global_

# to execute this experiment run:
# python train.py experiment=argoncrystal

defaults:
  - override /module/problem: argoncrystal
  - override /module/loss: mse
  - override /module/optimizer: adamw
  - override /module/scheduler: 1cycle
  - override /module/network: attention
  - override /trainer: gpu
  - override /callbacks: 
    - model_checkpoint
    - lr_monitor
    - fixed_seq_weights
    # - seq_len_scheduler
    - metrics_logger
    - tqdm_progress_bar
    - plot_energy_profile
    - plot_trajectory

trainer:
  max_epochs: 1000
  gradient_clip_val: 2.0
  gradient_clip_algorithm: value

callbacks:
  fixed_seq_weights:
    weights: [1., 1., 1., 1., 1., 1.]
  # seq_len_scheduler:
    # weights: [1., 1., 1., 1., 1.]
    # milestones: [50, 100]
    # init_len: 2

module:
  Delta_t: 1e-3
  
  use_dimensionless_for_loss: True

  optimizer:
    lr: 1e-4

  # scheduler:
  #   scheduler:
  #     pct_start: 0.05 
      # milestones: [50, 100, 150]
  
  network:
    h2h:
      input_dim: 28
      output_dim: 28
      # n_blocks: 2
      # n_linears_per_block: 3
      # in_features: 28

loggers:
  wandb:
    group: argoncrystal